{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    # mp_image = mp.Image.create_from_file('/path/to/image')\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)# Image is no longer writeable\n",
    "    results = model.detect(mp_image)                 # Make prediction      # Image is now writeable\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe.framework.formats import landmark_pb2\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "model_file = open('pose_landmarker_lite.task', \"rb\")\n",
    "model_data = model_file.read()\n",
    "model_file.close()\n",
    "    \n",
    "base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_poses =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the height and width to which each video frame will be resized in our dataset.\n",
    "IMAGE_HEIGHT , IMAGE_WIDTH = 256, 256\n",
    "\n",
    "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# Specify the directory containing the UCF50 dataset. \n",
    "\n",
    "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frames_extraction(video_path):\n",
    "    '''\n",
    "    This function will extract the required frames from a video after resizing and normalizing them.\n",
    "    Args:\n",
    "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
    "    Returns:\n",
    "        frames_list: A list containing the resized and normalized frames of the video.\n",
    "    '''\n",
    "\n",
    "    # Declare a list to store video frames.\n",
    "    frames_list = []\n",
    "    \n",
    "    # Read the Video File using the VideoCapture object.\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "        # Reading the frame from the video. \n",
    "        success, frame = video_reader.read() \n",
    "\n",
    "        # Check if Video frame is not successfully read then break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed height and width.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(resized_frame)\n",
    "    \n",
    "    # Release the VideoCapture object. \n",
    "    video_reader.release()\n",
    "\n",
    "    # Return the frames list.\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(pose_landmarks):\n",
    "    pose =[]\n",
    "    # if(len(results.pose_landmarks) == 2):\n",
    "    #     print(pose_landmarks_list[0])\n",
    "    #     print(pose_landmarks_list[1])\n",
    "    pose = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]).flatten() if pose_landmarks else np.zeros(33*4)\n",
    "    # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    # lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    # rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('train_dataset_train\\\\videos')\n",
    "\n",
    "# Actions that we try to detect\n",
    "\n",
    "actions = np.array(['cartwheel', 'catch', 'clap', 'climb', 'dive', 'draw_sword', 'dribble', 'fencing',\n",
    "                    'flic_flac', 'golf', 'handstand', 'hit', 'jump',\n",
    "                    'pick', 'pour', 'pullup', 'push', 'pushup', 'shoot_ball', 'sit', \n",
    "                    'situp', 'swing_baseball', 'sword_exercise', 'throw'])\n",
    "# actions = np.array(['cartwheel', 'catch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(20,132)))\n",
    "model_2.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model_2.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "model_2.add(Dense(32, activation='relu'))\n",
    "model_2.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.load_weights('modern_model_32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#тестирование модели без ожидания кадров > 20 (если не работает выполняем то что ниже)\n",
    "\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(\"D:\\\\hakaton\\Ставрополь\\\\train_dataset_train\\\\videos\\\\pullup\\\\Girl_does_12_pull-ups_pullup_u_cm_np1_ba_med_0.avi\")\n",
    "# Set mediapipe model\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    while cap.isOpened():\n",
    "        # number = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        # print(number)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        results = mediapipe_detection(frame, landmarker)\n",
    "\n",
    "        # Draw landmarks\n",
    "        annotated_image = draw_landmarks_on_image(frame, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        \n",
    "        for idx in range(len(results.pose_landmarks)):\n",
    "            pose_landmarks = results.pose_landmarks[idx]\n",
    "            keypoints = extract_keypoints(pose_landmarks)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-20:]\n",
    "            # keypoints = np.expand_dims(keypoints, axis=0)\n",
    "            # keypoints = np.expand_dims(keypoints, axis=0)\n",
    "            res = model_2.predict(np.expand_dims(sequence, axis=0))\n",
    "            \n",
    "            h, w, c = frame.shape\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in pose_landmarks:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            x_max = x_max + int(h/15)\n",
    "            y_max = y_max + int(h/15)\n",
    "            x_min = x_min - int(h/15)\n",
    "            y_min = y_min - int(h/15)\n",
    "            \n",
    "            action = actions[np.argmax(res)]\n",
    "                # predictions.append(np.argmax(res))\n",
    "            cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_image, ' '.join(action), (x_min,y_min),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', annotated_image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n",
      "1\n",
      "84.0\n",
      "2\n",
      "84.0\n",
      "3\n",
      "84.0\n",
      "4\n",
      "84.0\n",
      "5\n",
      "84.0\n",
      "6\n",
      "84.0\n",
      "7\n",
      "84.0\n",
      "8\n",
      "84.0\n",
      "9\n",
      "84.0\n",
      "10\n",
      "84.0\n",
      "11\n",
      "84.0\n",
      "12\n",
      "84.0\n",
      "13\n",
      "84.0\n",
      "14\n",
      "84.0\n",
      "15\n",
      "84.0\n",
      "16\n",
      "84.0\n",
      "17\n",
      "84.0\n",
      "18\n",
      "84.0\n",
      "19\n",
      "84.0\n",
      "20\n",
      "84.0\n",
      "21\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "84.0\n",
      "22\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "23\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "24\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "25\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "84.0\n",
      "26\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "27\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "28\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "29\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "30\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "31\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "32\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "33\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "34\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "35\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "36\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "37\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "38\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "39\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "40\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "41\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "42\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "43\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "44\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "45\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "46\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "47\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "48\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "49\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "50\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "51\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "52\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "53\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "54\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "55\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "56\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "57\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "58\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "59\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "60\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "61\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "62\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "63\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "64\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "65\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "66\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "67\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "68\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "69\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "70\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "71\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "72\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "73\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "74\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "75\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "76\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "77\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "78\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "79\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "80\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "81\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "84.0\n",
      "82\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n",
      "83\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "84.0\n"
     ]
    }
   ],
   "source": [
    "#тестирование модели с ожиданием кадров > 20\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "a= 0\n",
    "\n",
    "# Set mediapipe model\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    cap = cv2.VideoCapture(\"D:\\\\hakaton\\Ставрополь\\\\train_dataset_train\\\\videos\\\\pullup\\\\Girl_does_12_pull-ups_pullup_u_cm_np1_ba_med_0.avi\")\n",
    "    while cap.isOpened():\n",
    "        number = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        print(number)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        results = mediapipe_detection(frame, landmarker)\n",
    "\n",
    "        # Draw landmarks\n",
    "        annotated_image = draw_landmarks_on_image(frame, results)\n",
    "        a = a+1\n",
    "        print(a)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        \n",
    "        for idx in range(len(results.pose_landmarks)):\n",
    "            pose_landmarks = results.pose_landmarks[idx]\n",
    "            keypoints = extract_keypoints(pose_landmarks)\n",
    "            \n",
    "            sequence.append(keypoints)\n",
    "            # sequence = sequence[-20:]\n",
    "            # keypoints = np.expand_dims(keypoints, axis=0)\n",
    "            # keypoints = np.expand_dims(keypoints, axis=0)\n",
    "            if(len(sequence) >20):\n",
    "                sequence = sequence[-20:]\n",
    "                res = model_2.predict(np.expand_dims(sequence, axis=0))\n",
    "            \n",
    "                h, w, c = frame.shape\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lm in pose_landmarks:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                x_max = x_max + int(h/15)\n",
    "                y_max = y_max + int(h/15)\n",
    "                x_min = x_min - int(h/15)\n",
    "                y_min = y_min - int(h/15)\n",
    "            \n",
    "                action = actions[np.argmax(res)]\n",
    "                # predictions.append(np.argmax(res))\n",
    "                cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_image, ' '.join(action), (x_min,y_min),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', annotated_image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
